<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta charset="utf-8" />
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="x-ua-compatible" content="ie=edge">
    
    <title>Loss functions &#8212; Apache MXNet  documentation</title>

    <link rel="stylesheet" href="../../../../_static/basic.css" type="text/css" />
    <link rel="stylesheet" href="../../../../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/mxnet.css" />
    <link rel="stylesheet" href="../../../../_static/material-design-lite-1.3.0/material.blue-deep_orange.min.css" type="text/css" />
    <link rel="stylesheet" href="../../../../_static/sphinx_materialdesign_theme.css" type="text/css" />
    <link rel="stylesheet" href="../../../../_static/fontawesome/all.css" type="text/css" />
    <link rel="stylesheet" href="../../../../_static/fonts.css" type="text/css" />
    <script type="text/javascript" id="documentation_options" data-url_root="../../../../" src="../../../../_static/documentation_options.js"></script>
    <script type="text/javascript" src="../../../../_static/jquery.js"></script>
    <script type="text/javascript" src="../../../../_static/underscore.js"></script>
    <script type="text/javascript" src="../../../../_static/doctools.js"></script>
    <script type="text/javascript" src="../../../../_static/language_data.js"></script>
    <script type="text/javascript" src="../../../../_static/google_analytics.js"></script>
    <script async="async" type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true, "ignoreClass": "document", "processClass": "math|output_area"}})</script>
    <link rel="shortcut icon" href="../../../../_static/mxnet-icon.png"/>
    <link rel="index" title="Index" href="../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../search.html" /> 
  </head>
<body><header class="site-header" role="banner">
  <div class="wrapper">
      <a class="site-title" rel="author" href="/"><img
            src="../../../../_static/mxnet_logo.png" class="site-header-logo"></a>
    <nav class="site-nav">
      <input type="checkbox" id="nav-trigger" class="nav-trigger"/>
      <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
      </label>

      <div class="trigger">
        <a class="page-link" href="/get_started">Get Started</a>
        <a class="page-link" href="/blog">Blog</a>
        <a class="page-link" href="/features">Features</a>
        <a class="page-link" href="/ecosystem">Ecosystem</a>
        <a class="page-link page-current" href="/api">Docs & Tutorials</a>
        <a class="page-link" href="https://github.com/apache/incubator-mxnet">GitHub</a>
      </div>
    </nav>
  </div>
</header>
    <div class="mdl-layout mdl-js-layout mdl-layout--fixed-header mdl-layout--fixed-drawer"><header class="mdl-layout__header mdl-layout__header--waterfall ">
    <div class="mdl-layout__header-row">
        
        <nav class="mdl-navigation breadcrumb">
            <a class="mdl-navigation__link is-active">Loss functions</a>
        </nav>
        <div class="mdl-layout-spacer"></div>
        <nav class="mdl-navigation">
        
<form class="form-inline pull-sm-right" action="../../../../search.html" method="get">
      <div class="mdl-textfield mdl-js-textfield mdl-textfield--expandable mdl-textfield--floating-label mdl-textfield--align-right">
        <label id="quick-search-icon" class="mdl-button mdl-js-button mdl-button--icon"  for="waterfall-exp">
          <i class="material-icons">search</i>
        </label>
        <div class="mdl-textfield__expandable-holder">
          <input class="mdl-textfield__input" type="text" name="q"  id="waterfall-exp" placeholder="Search" />
          <input type="hidden" name="check_keywords" value="yes" />
          <input type="hidden" name="area" value="default" />
        </div>
      </div>
      <div class="mdl-tooltip" data-mdl-for="quick-search-icon">
      Quick search
      </div>
</form>
        
<a id="button-show-source"
    class="mdl-button mdl-js-button mdl-button--icon"
    href="../../../../_sources/tutorials/packages/gluon/loss/loss.ipynb" rel="nofollow">
  <i class="material-icons">code</i>
</a>
<div class="mdl-tooltip" data-mdl-for="button-show-source">
Show Source
</div>
        </nav>
    </div>
    <div class="mdl-layout__header-row header-links">
      <div class="mdl-layout-spacer"></div>
      <nav class="mdl-navigation">
      </nav>
    </div>
</header><header class="mdl-layout__drawer">      
    
      <div class="globaltoc">
        <span class="mdl-layout-title toc">Table Of Contents</span>
        
        
            
            <nav class="mdl-navigation">
                <ul>
<li class="toctree-l1"><a class="reference internal" href="../../../index.html">Python Tutorials</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../getting-started/index.html">Getting Started</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../getting-started/crash-course/index.html">Crash Course</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../getting-started/to-mxnet/index.html">Moving to MXNet from Other Frameworks</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../index.html">Packages</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../index.html">Gluon</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../ndarray/index.html">NDArray</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../symbol/index.html">Symbol</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../autograd/autograd.html">Automatic Differentiation</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../onnx/index.html">ONNX</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../performance/index.html">Performance</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../performance/compression/index.html">Compression</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../performance/backend/index.html">Accelerated Backend Tools</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../deploy/index.html">Deployment</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../deploy/export/index.html">Export</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../deploy/inference/index.html">Inference</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../deploy/run-on-aws/index.html">Run on AWS</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../extend/index.html">Customization</a><ul class="simple">
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api/index.html">Python API</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../../api/ndarray/index.html">mxnet.ndarray</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../../api/ndarray/mxnet.ndarray.NDArray.html">NDArray</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../api/ndarray/routines.html">Routines</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../api/ndarray/mxnet.ndarray.sparse.CSRNDArray.html">CSRNDArray</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../api/ndarray/mxnet.ndarray.sparse.RowSparseNDArray.html">RowSparseNDArray</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../api/ndarray/sparse_routines.html">Sparse routines</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../../api/gluon/index.html">mxnet.gluon</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../../api/gluon/nn.html">nn and contrib.nn</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../api/gluon/rnn.html">rnn and contrib.rnn</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../api/gluon/mxnet.gluon.loss.html"><code class="docutils literal notranslate"><span class="pre">loss</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../api/gluon/mxnet.gluon.parameter.html">Parameter</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../api/gluon/mxnet.gluon.Trainer.html">Trainer</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../api/gluon/mxnet.gluon.data.html">data</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../api/gluon/mxnet.gluon.data.vision.html">data.vision</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../api/gluon/mxnet.gluon.model_zoo.html">model_zoo.vision</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../api/gluon/mxnet.gluon.utils.html">utils</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../../api/gluon-related/index.html">Gluon related modules</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../../api/gluon-related/mxnet.autograd.html">mxnet.autograd</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../api/gluon-related/mxnet.image.html">mxnet.image</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../api/gluon-related/mxnet.io.html">mxnet.io</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../api/gluon-related/mxnet.recordio.html">mxnet.recordio</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../api/gluon-related/mxnet.kvstore.html">mxnet.kvstore</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../api/gluon-related/mxnet.optimizer.html">mxnet.optimizer</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../api/gluon-related/mxnet.random.html">mxnet.random</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../api/gluon-related/mxnet.profiler.html">mxnet.profiler</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../api/gluon-related/mxnet.context.html">mxnet.context</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../api/gluon-related/mxnet.initializer.html">mxnet.initializer</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../api/gluon-related/mxnet.lr_scheduler.html">mxnet.lr_scheduler</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../api/gluon-related/mxnet.metric.html">mxnet.metric</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../../api/symbol/index.html">mxnet.symbol</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../../api/symbol/mxnet.symbol.Symbol.html">Symbol</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../api/symbol/mxnet.symbol.linalg.html">mxnet.linalg</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../../api/symbol-related/index.html">Symbol related modules</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../../api/symbol-related/mxnet.callback.html">mxnet.callback</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../api/symbol-related/mxnet.module.html">mxnet.module</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../api/symbol-related/mxnet.monitor.html">mxnet.monitor</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../api/symbol-related/mxnet.visualization.html">mxnet.visualization</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../../api/advanced/index.html">Advanced modules</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../../api/advanced/mxnet.executor.html">mxnet.executor</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../api/advanced/mxnet.kvstore_server.html">mxnet.kvstore_server</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../api/advanced/mxnet.engine.html">mxnet.engine</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../api/advanced/mxnet.executor_manager.html">mxnet.executor_manager</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../api/advanced/mxnet.rtc.html">mxnet.rtc</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../api/advanced/mxnet.test_utils.html">mxnet.test_utils</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../api/advanced/mxnet.util.html">mxnet.util</a></li>
</ul>
</li>
</ul>
</li>
</ul>

            </nav>
        
        </div>
    
</header>
        <main class="mdl-layout__content" tabIndex="0">

	<script type="text/javascript" src="../../../../_static/sphinx_materialdesign_theme.js "></script>
    <header class="mdl-layout__drawer">      
    
      <div class="globaltoc">
        <span class="mdl-layout-title toc">Table Of Contents</span>
        
        
            
            <nav class="mdl-navigation">
                <ul>
<li class="toctree-l1"><a class="reference internal" href="../../../index.html">Python Tutorials</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../getting-started/index.html">Getting Started</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../getting-started/crash-course/index.html">Crash Course</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../getting-started/to-mxnet/index.html">Moving to MXNet from Other Frameworks</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../index.html">Packages</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../index.html">Gluon</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../ndarray/index.html">NDArray</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../symbol/index.html">Symbol</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../autograd/autograd.html">Automatic Differentiation</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../onnx/index.html">ONNX</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../performance/index.html">Performance</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../performance/compression/index.html">Compression</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../performance/backend/index.html">Accelerated Backend Tools</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../deploy/index.html">Deployment</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../deploy/export/index.html">Export</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../deploy/inference/index.html">Inference</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../deploy/run-on-aws/index.html">Run on AWS</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../extend/index.html">Customization</a><ul class="simple">
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api/index.html">Python API</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../../api/ndarray/index.html">mxnet.ndarray</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../../api/ndarray/mxnet.ndarray.NDArray.html">NDArray</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../api/ndarray/routines.html">Routines</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../api/ndarray/mxnet.ndarray.sparse.CSRNDArray.html">CSRNDArray</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../api/ndarray/mxnet.ndarray.sparse.RowSparseNDArray.html">RowSparseNDArray</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../api/ndarray/sparse_routines.html">Sparse routines</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../../api/gluon/index.html">mxnet.gluon</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../../api/gluon/nn.html">nn and contrib.nn</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../api/gluon/rnn.html">rnn and contrib.rnn</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../api/gluon/mxnet.gluon.loss.html"><code class="docutils literal notranslate"><span class="pre">loss</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../api/gluon/mxnet.gluon.parameter.html">Parameter</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../api/gluon/mxnet.gluon.Trainer.html">Trainer</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../api/gluon/mxnet.gluon.data.html">data</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../api/gluon/mxnet.gluon.data.vision.html">data.vision</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../api/gluon/mxnet.gluon.model_zoo.html">model_zoo.vision</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../api/gluon/mxnet.gluon.utils.html">utils</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../../api/gluon-related/index.html">Gluon related modules</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../../api/gluon-related/mxnet.autograd.html">mxnet.autograd</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../api/gluon-related/mxnet.image.html">mxnet.image</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../api/gluon-related/mxnet.io.html">mxnet.io</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../api/gluon-related/mxnet.recordio.html">mxnet.recordio</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../api/gluon-related/mxnet.kvstore.html">mxnet.kvstore</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../api/gluon-related/mxnet.optimizer.html">mxnet.optimizer</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../api/gluon-related/mxnet.random.html">mxnet.random</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../api/gluon-related/mxnet.profiler.html">mxnet.profiler</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../api/gluon-related/mxnet.context.html">mxnet.context</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../api/gluon-related/mxnet.initializer.html">mxnet.initializer</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../api/gluon-related/mxnet.lr_scheduler.html">mxnet.lr_scheduler</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../api/gluon-related/mxnet.metric.html">mxnet.metric</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../../api/symbol/index.html">mxnet.symbol</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../../api/symbol/mxnet.symbol.Symbol.html">Symbol</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../api/symbol/mxnet.symbol.linalg.html">mxnet.linalg</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../../api/symbol-related/index.html">Symbol related modules</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../../api/symbol-related/mxnet.callback.html">mxnet.callback</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../api/symbol-related/mxnet.module.html">mxnet.module</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../api/symbol-related/mxnet.monitor.html">mxnet.monitor</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../api/symbol-related/mxnet.visualization.html">mxnet.visualization</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../../api/advanced/index.html">Advanced modules</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../../api/advanced/mxnet.executor.html">mxnet.executor</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../api/advanced/mxnet.kvstore_server.html">mxnet.kvstore_server</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../api/advanced/mxnet.engine.html">mxnet.engine</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../api/advanced/mxnet.executor_manager.html">mxnet.executor_manager</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../api/advanced/mxnet.rtc.html">mxnet.rtc</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../api/advanced/mxnet.test_utils.html">mxnet.test_utils</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../api/advanced/mxnet.util.html">mxnet.util</a></li>
</ul>
</li>
</ul>
</li>
</ul>

            </nav>
        
        </div>
    
</header>

    <div class="document">
        <div class="page-content">
        
  
<style>
/* CSS for nbsphinx extension */

/* remove conflicting styling from Sphinx themes */
div.nbinput,
div.nbinput div.prompt,
div.nbinput div.input_area,
div.nbinput div[class*=highlight],
div.nbinput div[class*=highlight] pre,
div.nboutput,
div.nbinput div.prompt,
div.nbinput div.output_area,
div.nboutput div[class*=highlight],
div.nboutput div[class*=highlight] pre {
    background: none;
    border: none;
    padding: 0 0;
    margin: 0;
    box-shadow: none;
}

/* avoid gaps between output lines */
div.nboutput div[class*=highlight] pre {
    line-height: normal;
}

/* input/output containers */
div.nbinput,
div.nboutput {
    display: -webkit-flex;
    display: flex;
    align-items: flex-start;
    margin: 0;
    width: 100%;
}
@media (max-width: 540px) {
    div.nbinput,
    div.nboutput {
        flex-direction: column;
    }
}

/* input container */
div.nbinput {
    padding-top: 5px;
}

/* last container */
div.nblast {
    padding-bottom: 5px;
}

/* input prompt */
div.nbinput div.prompt pre {
    color: #307FC1;
}

/* output prompt */
div.nboutput div.prompt pre {
    color: #BF5B3D;
}

/* all prompts */
div.nbinput div.prompt,
div.nboutput div.prompt {
    min-width: 7ex;
    padding-top: 0.4em;
    padding-right: 0.4em;
    text-align: right;
    flex: 0;
}
@media (max-width: 540px) {
    div.nbinput div.prompt,
    div.nboutput div.prompt {
        text-align: left;
        padding: 0.4em;
    }
    div.nboutput div.prompt.empty {
        padding: 0;
    }
}

/* disable scrollbars on prompts */
div.nbinput div.prompt pre,
div.nboutput div.prompt pre {
    overflow: hidden;
}

/* input/output area */
div.nbinput div.input_area,
div.nboutput div.output_area {
    padding: 0.4em;
    -webkit-flex: 1;
    flex: 1;
    overflow: auto;
}
@media (max-width: 540px) {
    div.nbinput div.input_area,
    div.nboutput div.output_area {
        width: 100%;
    }
}

/* input area */
div.nbinput div.input_area {
    border: 1px solid #e0e0e0;
    border-radius: 2px;
    background: #f5f5f5;
}

/* override MathJax center alignment in output cells */
div.nboutput div[class*=MathJax] {
    text-align: left !important;
}

/* override sphinx.ext.imgmath center alignment in output cells */
div.nboutput div.math p {
    text-align: left;
}

/* standard error */
div.nboutput div.output_area.stderr {
    background: #fdd;
}

/* ANSI colors */
.ansi-black-fg { color: #3E424D; }
.ansi-black-bg { background-color: #3E424D; }
.ansi-black-intense-fg { color: #282C36; }
.ansi-black-intense-bg { background-color: #282C36; }
.ansi-red-fg { color: #E75C58; }
.ansi-red-bg { background-color: #E75C58; }
.ansi-red-intense-fg { color: #B22B31; }
.ansi-red-intense-bg { background-color: #B22B31; }
.ansi-green-fg { color: #00A250; }
.ansi-green-bg { background-color: #00A250; }
.ansi-green-intense-fg { color: #007427; }
.ansi-green-intense-bg { background-color: #007427; }
.ansi-yellow-fg { color: #DDB62B; }
.ansi-yellow-bg { background-color: #DDB62B; }
.ansi-yellow-intense-fg { color: #B27D12; }
.ansi-yellow-intense-bg { background-color: #B27D12; }
.ansi-blue-fg { color: #208FFB; }
.ansi-blue-bg { background-color: #208FFB; }
.ansi-blue-intense-fg { color: #0065CA; }
.ansi-blue-intense-bg { background-color: #0065CA; }
.ansi-magenta-fg { color: #D160C4; }
.ansi-magenta-bg { background-color: #D160C4; }
.ansi-magenta-intense-fg { color: #A03196; }
.ansi-magenta-intense-bg { background-color: #A03196; }
.ansi-cyan-fg { color: #60C6C8; }
.ansi-cyan-bg { background-color: #60C6C8; }
.ansi-cyan-intense-fg { color: #258F8F; }
.ansi-cyan-intense-bg { background-color: #258F8F; }
.ansi-white-fg { color: #C5C1B4; }
.ansi-white-bg { background-color: #C5C1B4; }
.ansi-white-intense-fg { color: #A1A6B2; }
.ansi-white-intense-bg { background-color: #A1A6B2; }

.ansi-default-inverse-fg { color: #FFFFFF; }
.ansi-default-inverse-bg { background-color: #000000; }

.ansi-bold { font-weight: bold; }
.ansi-underline { text-decoration: underline; }

/* Some additional styling taken form the Jupyter notebook CSS */
div.rendered_html table {
  border: none;
  border-collapse: collapse;
  border-spacing: 0;
  color: black;
  font-size: 12px;
  table-layout: fixed;
}
div.rendered_html thead {
  border-bottom: 1px solid black;
  vertical-align: bottom;
}
div.rendered_html tr,
div.rendered_html th,
div.rendered_html td {
  text-align: right;
  vertical-align: middle;
  padding: 0.5em 0.5em;
  line-height: normal;
  white-space: normal;
  max-width: none;
  border: none;
}
div.rendered_html th {
  font-weight: bold;
}
div.rendered_html tbody tr:nth-child(odd) {
  background: #f5f5f5;
}
div.rendered_html tbody tr:hover {
  background: rgba(66, 165, 245, 0.2);
}
</style>
<div class="section" id="Loss-functions">
<h1>Loss functions<a class="headerlink" href="#Loss-functions" title="Permalink to this headline">¶</a></h1>
<p>Loss functions are used to train neural networks and to compute the difference between output and target variable. A critical component of training neural networks is the loss function. A loss function is a quantative measure of how bad the predictions of the network are when compared to ground truth labels. Given this score, a network can improve by iteratively updating its weights to minimise this loss. Some tasks use a combination of multiple loss functions, but often you’ll just use one.
MXNet Gluon provides a number of the most commonly used loss functions, and you’ll choose certain loss functions depending on your network and task. Some common task and loss function pairs include:</p>
<ul class="simple">
<li><p>regression: <a class="reference external" href="https://beta.mxnet.io/api/gluon/_autogen/mxnet.gluon.loss.L1Loss.html">L1Loss</a>, <a class="reference external" href="https://beta.mxnet.io/api/gluon/_autogen/mxnet.gluon.loss.L2Loss.html">L2Loss</a></p></li>
<li><p>classification: <a class="reference external" href="https://beta.mxnet.io/api/gluon/_autogen/mxnet.gluon.loss.SigmoidBinaryCrossEntropyLoss.html">SigmoidBinaryCrossEntropyLoss</a>, <a class="reference external" href="https://beta.mxnet.io/api/gluon/_autogen/mxnet.gluon.loss.SoftmaxBinaryCrossEntropyLoss.html">SoftmaxBinaryCrossEntropyLoss</a></p></li>
<li><p>embeddings: <a class="reference external" href="https://beta.mxnet.io/api/gluon/_autogen/mxnet.gluon.loss.HingeLoss.html">HingeLoss</a></p></li>
</ul>
<p>We’ll first import the modules, where the <code class="docutils literal notranslate"><span class="pre">mxnet.gluon.loss</span></code> module is imported as <code class="docutils literal notranslate"><span class="pre">gloss</span></code> to avoid the commonly used name <code class="docutils literal notranslate"><span class="pre">loss</span></code>.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-python notranslate"><div class="highlight"><pre>
<span></span><span class="kn">from</span> <span class="nn">IPython</span> <span class="kn">import</span> <span class="n">display</span>
<span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">import</span> <span class="nn">mxnet</span> <span class="kn">as</span> <span class="nn">mx</span>
<span class="kn">from</span> <span class="nn">mxnet</span> <span class="kn">import</span> <span class="n">nd</span><span class="p">,</span> <span class="n">autograd</span>
<span class="kn">from</span> <span class="nn">mxnet.gluon</span> <span class="kn">import</span> <span class="n">nn</span><span class="p">,</span> <span class="n">loss</span> <span class="k">as</span> <span class="n">gloss</span>
</pre></div>
</div>
</div>
<div class="section" id="Basic-Usages">
<h2>Basic Usages<a class="headerlink" href="#Basic-Usages" title="Permalink to this headline">¶</a></h2>
<p>Now let’s create an instance of the <span class="math notranslate nohighlight">\(\ell_2\)</span> loss, which is commonly used in regression tasks.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-python notranslate"><div class="highlight"><pre>
<span></span><span class="n">loss</span> <span class="o">=</span> <span class="n">gloss</span><span class="o">.</span><span class="n">L2Loss</span><span class="p">()</span>
</pre></div>
</div>
</div>
<p>And then feed two inputs to compute the elementwise loss values.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-python notranslate"><div class="highlight"><pre>
<span></span><span class="n">x</span> <span class="o">=</span> <span class="n">nd</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">2</span><span class="p">,))</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">nd</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">2</span><span class="p">,))</span> <span class="o">*</span> <span class="mi">2</span>
<span class="n">loss</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p>These values should be equal to the math definition: <span class="math notranslate nohighlight">\(0.5\|x-y\|^2\)</span>.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-python notranslate"><div class="highlight"><pre>
<span></span><span class="o">.</span><span class="mi">5</span> <span class="o">*</span> <span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">y</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span>
</pre></div>
</div>
</div>
<p>Next we show how to use a loss function to compute gradients.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-python notranslate"><div class="highlight"><pre>
<span></span><span class="n">X</span> <span class="o">=</span> <span class="n">nd</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
<span class="n">net</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="n">net</span><span class="o">.</span><span class="n">initialize</span><span class="p">()</span>
<span class="k">with</span> <span class="n">autograd</span><span class="o">.</span><span class="n">record</span><span class="p">():</span>
    <span class="n">l</span> <span class="o">=</span>  <span class="n">loss</span><span class="p">(</span><span class="n">net</span><span class="p">(</span><span class="n">X</span><span class="p">),</span> <span class="n">y</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">l</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p>We can compute the gradients w.r.t. the loss function.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-python notranslate"><div class="highlight"><pre>
<span></span><span class="n">l</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
<span class="k">print</span><span class="p">(</span><span class="n">net</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">grad</span><span class="p">())</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="Loss-functions">
<h2>Loss functions<a class="headerlink" href="#Loss-functions" title="Permalink to this headline">¶</a></h2>
<p>Most commonly used loss functions can be divided into 2 categories: regression and classification.</p>
<p>Let’s first visualize several regression losses. We visualize the loss values versus the predicted values with label values fixed to be 0.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-python notranslate"><div class="highlight"><pre>
<span></span><span class="k">def</span> <span class="nf">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="n">display</span><span class="o">.</span><span class="n">set_matplotlib_formats</span><span class="p">(</span><span class="s1">&#39;svg&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">(),</span> <span class="n">y</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">())</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;x&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;loss&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="k">def</span> <span class="nf">show_regression_loss</span><span class="p">(</span><span class="n">loss</span><span class="p">):</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">nd</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="o">.</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">loss</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">nd</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
    <span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

</pre></div>
</div>
</div>
<p>Then plot the classification losses with label values fixed to be 1.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-python notranslate"><div class="highlight"><pre>
<span></span><span class="k">def</span> <span class="nf">show_classification_loss</span><span class="p">(</span><span class="n">loss</span><span class="p">):</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">nd</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="o">.</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">loss</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">nd</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
    <span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="L1-Loss">
<h3><a class="reference external" href="https://mxnet.incubator.apache.org/api/python/gluon/loss.html#mxnet.gluon.loss.L1Loss">L1 Loss</a><a class="headerlink" href="#L1-Loss" title="Permalink to this headline">¶</a></h3>
<p>L1 Loss, also called Mean Absolute Error, computes the sum of absolute distance between target values and the output of the neural network. It is defined as:</p>
<div class="math notranslate nohighlight">
\[L = \sum_i \vert {label}_i - {pred}_i \vert.\]</div>
<p>It is a non-smooth function that can lead to non-convergence. It creates the same gradient for small and large loss values, which can be problematic for the learning process.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-python notranslate"><div class="highlight"><pre>
<span></span><span class="n">show_regression_loss</span><span class="p">(</span><span class="n">gloss</span><span class="o">.</span><span class="n">L1Loss</span><span class="p">())</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="L2-Loss">
<h3><a class="reference external" href="https://mxnet.incubator.apache.org/api/python/gluon/loss.html#mxnet.gluon.loss.L2Loss">L2 Loss</a><a class="headerlink" href="#L2-Loss" title="Permalink to this headline">¶</a></h3>
<p>L2Loss, also called Mean Squared Error, is a regression loss function that computes the squared distances between the target values and the output of the neural network. It is defined as:</p>
<div class="math notranslate nohighlight">
\[L = \frac{1}{2} \sum_i \vert {label}_i - {pred}_i \vert^2.\]</div>
<p>Compared to L1, L2 loss it is a smooth function and it creates larger gradients for large loss values. However due to the squaring it puts high weight on outliers.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-python notranslate"><div class="highlight"><pre>
<span></span><span class="n">show_regression_loss</span><span class="p">(</span><span class="n">gloss</span><span class="o">.</span><span class="n">L2Loss</span><span class="p">())</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="Huber-Loss">
<h3><a class="reference external" href="https://mxnet.incubator.apache.org/api/python/gluon/loss.html#mxnet.gluon.loss.HuberLosss">Huber Loss</a><a class="headerlink" href="#Huber-Loss" title="Permalink to this headline">¶</a></h3>
<p>HuberLoss combines advantages of L1 and L2 loss. It calculates a smoothed L1 loss that is equal to L1 if the absolute error exceeds a threshold</p>
<div class="math notranslate nohighlight">
\[\rho\]</div>
<p>, otherwise it is equal to L2. It is defined as:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{split}L = \sum_i \begin{cases} \frac{1}{2 {rho}} ({label}_i - {pred}_i)^2 &amp;
                   \text{ if } |{label}_i - {pred}_i| &lt; {rho} \\
                   |{label}_i - {pred}_i| - \frac{{rho}}{2} &amp;
                   \text{ otherwise }
    \end{cases}\end{split}\end{split}\]</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-python notranslate"><div class="highlight"><pre>
<span></span><span class="n">show_regression_loss</span><span class="p">(</span><span class="n">gloss</span><span class="o">.</span><span class="n">HuberLoss</span><span class="p">(</span><span class="n">rho</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>
</pre></div>
</div>
</div>
<p>An example of where Huber Loss is used can be found in <a class="reference external" href="https://openai.com/blog/openai-baselines-dqn/">Deep Q Network</a>.</p>
</div>
<div class="section" id="Cross-Entropy-Loss-with-Sigmoid">
<h3><a class="reference external" href="https://mxnet.incubator.apache.org/api/python/gluon/loss.html#mxnet.gluon.loss.SigmoidBinaryCrossEntropyLoss">Cross Entropy Loss with Sigmoid</a><a class="headerlink" href="#Cross-Entropy-Loss-with-Sigmoid" title="Permalink to this headline">¶</a></h3>
<p>Binary Cross Entropy is a loss function used for binary classification problems e.g. classifying images into 2 classes. Cross entropy measures the difference between two probability distributions and it is defined as:</p>
<div class="math notranslate nohighlight">
\[\sum_i -{(y\log(p) + (1 - y)\log(1 - p))}\]</div>
<p>Before the loss is computed a sigmoid activation is applied per default. If your network has <code class="docutils literal notranslate"><span class="pre">sigmoid</span></code> activation as last layer, then you need set <code class="docutils literal notranslate"><span class="pre">from_sigmoid</span></code> to False, to avoid applying the sigmoid function twice.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-python notranslate"><div class="highlight"><pre>
<span></span><span class="n">show_classification_loss</span><span class="p">(</span><span class="n">gloss</span><span class="o">.</span><span class="n">SigmoidBinaryCrossEntropyLoss</span><span class="p">())</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="Cross-Entropy-Loss-with-Softmax">
<h3><a class="reference external" href="https://mxnet.incubator.apache.org/api/python/gluon/loss.html#mxnet.gluon.loss.SoftmaxCrossEntropyLoss">Cross Entropy Loss with Softmax</a><a class="headerlink" href="#Cross-Entropy-Loss-with-Softmax" title="Permalink to this headline">¶</a></h3>
<p>In classification, we often apply the softmax operator to the predicted outputs to obtain prediction probabilities, and then apply the cross entropy loss against the true labels:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}\begin{aligned}p = \softmax({pred})\\L = -\sum_i \sum_j {label}_j \log p_{ij}\end{aligned}\end{align}\end{split}\]</div>
<p>Running these two steps one-by-one, however, may lead to numerical instabilities. The <code class="docutils literal notranslate"><span class="pre">loss</span></code> module provides a single operators with softmax and cross entropy fused to avoid such problem.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-python notranslate"><div class="highlight"><pre>
<span></span><span class="n">loss</span> <span class="o">=</span> <span class="n">gloss</span><span class="o">.</span><span class="n">SoftmaxCrossEntropyLoss</span><span class="p">()</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">nd</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">],</span> <span class="p">[</span><span class="mi">8</span><span class="p">,</span> <span class="mi">2</span><span class="p">]])</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">nd</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
<span class="n">loss</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="Hinge-Loss">
<h3><a class="reference external" href="https://mxnet.incubator.apache.org/api/python/gluon/loss.html#mxnet.gluon.loss.HingeLoss">Hinge Loss</a><a class="headerlink" href="#Hinge-Loss" title="Permalink to this headline">¶</a></h3>
<p>Commonly used in Support Vector Machines (SVMs), Hinge Loss is used to additionally penalize predictions that are correct but fall within a margin between classes (the region around a decision boundary). Unlike <code class="docutils literal notranslate"><span class="pre">SoftmaxCrossEntropyLoss</span></code>, it’s rarely used for neural network training. It is defined as:</p>
<div class="math notranslate nohighlight">
\[L = \sum_i max(0, {margin} - {pred}_i \cdot {label}_i)\]</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-python notranslate"><div class="highlight"><pre>
<span></span><span class="n">show_classification_loss</span><span class="p">(</span><span class="n">gloss</span><span class="o">.</span><span class="n">HingeLoss</span><span class="p">())</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="Logistic-Loss">
<h3><a class="reference external" href="https://mxnet.incubator.apache.org/versions/master/api/python/gluon/loss.html#mxnet.gluon.loss.LogisticLoss">Logistic Loss</a><a class="headerlink" href="#Logistic-Loss" title="Permalink to this headline">¶</a></h3>
<p>The Logistic Loss function computes the performance of binary classification models.</p>
<div class="math notranslate nohighlight">
\[L = \sum_i \log(1 + \exp(- {pred}_i \cdot {label}_i))\]</div>
<p>The log loss decreases the closer the prediction is to the actual label. It is sensitive to outliers, because incorrectly classified points are penalized more.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-python notranslate"><div class="highlight"><pre>
<span></span><span class="n">show_classification_loss</span><span class="p">(</span><span class="n">gloss</span><span class="o">.</span><span class="n">LogisticLoss</span><span class="p">())</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="Kullback-Leibler-Divergence-Loss">
<h3><a class="reference external" href="https://mxnet.incubator.apache.org/versions/master/api/python/gluon/loss.html#mxnet.gluon.loss.KLDivLoss">Kullback-Leibler Divergence Loss</a><a class="headerlink" href="#Kullback-Leibler-Divergence-Loss" title="Permalink to this headline">¶</a></h3>
<p>The Kullback-Leibler divergence loss measures the divergence between two probability distributions by calculating the difference between cross entropy and entropy. It takes as input the probability of predicted label and the probability of true label.</p>
<div class="math notranslate nohighlight">
\[L = \sum_i {label}_i * \big[\log({label}_i) - {pred}_i\big]\]</div>
<p>The loss is large, if the predicted probability distribution is far from the ground truth probability distribution. KL divergence is an asymmetric measure. KL divergence loss can be used in Variational Autoencoders (VAEs), and reinforcement learning policy networks such as Trust Region Policy Optimization (TRPO)</p>
<p>For instance, in the following example we get a KL divergence of 0.02. We set <code class="docutils literal notranslate"><span class="pre">from_logits=False</span></code>, so the loss functions will apply <code class="docutils literal notranslate"><span class="pre">log_softmax</span></code> on the network output, before computing the KL divergence.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">output</span> <span class="o">=</span> <span class="n">mx</span><span class="o">.</span><span class="n">nd</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">0.39056206</span><span class="p">,</span> <span class="mf">1.3068528</span><span class="p">,</span> <span class="mf">0.39056206</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.30258512</span><span class="p">]])</span>
<span class="k">print</span><span class="p">(</span><span class="s1">&#39;output.softmax(): {}&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">output</span><span class="o">.</span><span class="n">softmax</span><span class="p">()</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">()</span><span class="o">.</span><span class="n">tolist</span><span class="p">()))</span>
<span class="n">target_dist</span> <span class="o">=</span> <span class="n">mx</span><span class="o">.</span><span class="n">nd</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.4</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">]])</span>
<span class="n">loss_fn</span> <span class="o">=</span> <span class="n">gloss</span><span class="o">.</span><span class="n">KLDivLoss</span><span class="p">(</span><span class="n">from_logits</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">loss_fn</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">target_dist</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s1">&#39;loss (kl divergence): {}&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">loss</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">()</span><span class="o">.</span><span class="n">tolist</span><span class="p">()))</span>
</pre></div>
</div>
</div>
<div class="section" id="Triplet-Loss">
<h3><a class="reference external" href="https://mxnet.incubator.apache.org/versions/master/api/python/gluon/loss.html#mxnet.gluon.loss.TripletLoss">Triplet Loss</a><a class="headerlink" href="#Triplet-Loss" title="Permalink to this headline">¶</a></h3>
<p>Triplet loss takes three input arrays and measures the relative similarity. It takes a positive and negative input and the anchor.</p>
<div class="math notranslate nohighlight">
\[L = \sum_i \max(\Vert {pos_i}_i - {pred} \Vert_2^2 -
    \Vert {neg_i}_i - {pred} \Vert_2^2 + {margin}, 0)\]</div>
<div class="line-block">
<div class="line">The loss function minimizes the distance between similar inputs and maximizes the distance between dissimilar ones.</div>
<div class="line">In the case of learning embeddings for images of characters, the network may get as input the following 3 images:</div>
</div>
<p><img alt="triplet_loss" src="tutorials/packages/gluon/loss/triplet_loss.png" /></p>
<p>The network would learn to minimize the distance between the two <code class="docutils literal notranslate"><span class="pre">A</span></code>’s and maximize the distance between <code class="docutils literal notranslate"><span class="pre">A</span></code> and <code class="docutils literal notranslate"><span class="pre">Z</span></code>.</p>
</div>
<div class="section" id="CTC-Loss">
<h3><a class="reference external" href="https://mxnet.incubator.apache.org/versions/master/api/python/gluon/loss.html#mxnet.gluon.loss.CTCLoss">CTC Loss</a><a class="headerlink" href="#CTC-Loss" title="Permalink to this headline">¶</a></h3>
<p>CTC Loss is the <a class="reference external" href="https://distill.pub/2017/ctc/">connectionist temporal classification loss</a> . It is used to train recurrent neural networks with variable time dimension. It learns the alignment and labelling of input sequences. It takes a sequence as input and gives probabilities for each timestep. For instance, in the following image the word is not well aligned with the 5 timesteps because of the different sizes of characters. CTC Loss finds for each timestep the highest probability
e.g. <code class="docutils literal notranslate"><span class="pre">t1</span></code> presents with high probability a <code class="docutils literal notranslate"><span class="pre">C</span></code>. It combines the highest probapilities and returns the best path decoding. For an in-depth tutorial on how to use CTC-Loss in MXNet, check out this <a class="reference external" href="https://github.com/apache/incubator-mxnet/tree/master/example/ctc">example</a>.</p>
<p><img alt="ctc_loss" src="tutorials/packages/gluon/loss/ctc_loss.png" /></p>
</div>
<div class="section" id="Cosine-Embedding-Loss">
<h3><a class="reference external" href="https://mxnet.incubator.apache.org/versions/master/api/python/gluon/loss.html#mxnet.gluon.loss.CosineEmbeddingLoss">Cosine Embedding Loss</a><a class="headerlink" href="#Cosine-Embedding-Loss" title="Permalink to this headline">¶</a></h3>
<p>The cosine embedding loss computes the cosine distance between two input vectors.</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{split}L = \sum_i \begin{cases} 1 - {cos\_sim({input1}_i, {input2}_i)} &amp; \text{ if } {label}_i = 1\\
                 {cos\_sim({input1}_i, {input2}_i)} &amp; \text{ if } {label}_i = -1 \end{cases}\\
cos\_sim(input1, input2) = \frac{{input1}_i.{input2}_i}{||{input1}_i||.||{input2}_i||}\end{split}\end{split}\]</div>
<div class="line-block">
<div class="line">Cosine distance measures the similarity between two arrays given a label and is typically used for learning nonlinear embeddings.</div>
<div class="line">For instance, in the following code example we measure the similarity between the input vectors <code class="docutils literal notranslate"><span class="pre">x</span></code> and <code class="docutils literal notranslate"><span class="pre">y</span></code>. Since they are the same the label equals <code class="docutils literal notranslate"><span class="pre">1</span></code>. The loss function returns</div>
</div>
<blockquote>
<div><div class="math notranslate nohighlight">
\[\sum_i 1 - {cos\_sim({input1}_i, {input2}_i)}\]</div>
<p>which is equal <code class="docutils literal notranslate"><span class="pre">0</span></code>.</p>
</div></blockquote>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-python notranslate"><div class="highlight"><pre>
<span></span><span class="n">x</span> <span class="o">=</span> <span class="n">mx</span><span class="o">.</span><span class="n">nd</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">])</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">mx</span><span class="o">.</span><span class="n">nd</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">])</span>
<span class="n">label</span> <span class="o">=</span> <span class="n">mx</span><span class="o">.</span><span class="n">nd</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">gloss</span><span class="o">.</span><span class="n">CosineEmbeddingLoss</span><span class="p">()</span>
<span class="k">print</span><span class="p">(</span><span class="n">loss</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">,</span><span class="n">label</span><span class="p">))</span>
</pre></div>
</div>
</div>
<p>Now let’s make <code class="docutils literal notranslate"><span class="pre">y</span></code> the opposite of <code class="docutils literal notranslate"><span class="pre">x</span></code>, so we set the label <code class="docutils literal notranslate"><span class="pre">-1</span></code> and the function will return</p>
<div class="math notranslate nohighlight">
\[\sum_i cos\_sim(input1, input2)\]</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-python notranslate"><div class="highlight"><pre>
<span></span><span class="n">x</span> <span class="o">=</span> <span class="n">mx</span><span class="o">.</span><span class="n">nd</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">])</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">mx</span><span class="o">.</span><span class="n">nd</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">])</span>
<span class="n">label</span> <span class="o">=</span> <span class="n">mx</span><span class="o">.</span><span class="n">nd</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">gloss</span><span class="o">.</span><span class="n">CosineEmbeddingLoss</span><span class="p">()</span>
<span class="k">print</span><span class="p">(</span><span class="n">loss</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">,</span><span class="n">label</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="PoissonNLLLoss">
<h3><a class="reference external" href="https://mxnet.incubator.apache.org/versions/master/api/python/gluon/loss.html#mxnet.gluon.loss.PoissonNLLLoss">PoissonNLLLoss</a><a class="headerlink" href="#PoissonNLLLoss" title="Permalink to this headline">¶</a></h3>
<p>Poisson distribution is widely used for modelling count data. It is defined as:</p>
<div class="math notranslate nohighlight">
\[f(x) = \frac{\mu ^ {\kern 0.08 em x} e ^ {-\mu}} {x!} \qquad \qquad x = 0,1,2 , \ldots \,.\]</div>
<p>For instance, the count of cars in road traffic approximately follows a Poisson distribution. Using an ordinary least squares model for Poisson distributed data would not work well because of two reasons: - count data cannot be negative - variance may not be constant</p>
<p>Instead we can use a Poisson regression model, also known as log-linear model. Thereby the Poisson incident rate</p>
<div class="math notranslate nohighlight">
\[\mu\]</div>
<p>is modelled by a linear combination of unknown parameters. We can then use the PoissonNLLLoss which calculates the negative log likelihood for a target that follows a Poisson distribution.</p>
<div class="math notranslate nohighlight">
\[L = \text{pred} - \text{target} * \log(\text{pred}) +\log(\text{target!})\]</div>
</div>
</div>
<div class="section" id="Advanced:-Weighted-Loss">
<h2>Advanced: Weighted Loss<a class="headerlink" href="#Advanced:-Weighted-Loss" title="Permalink to this headline">¶</a></h2>
<p>Some examples in a batch may be more important than others. We can apply weights to individual examples during the forward pass of the loss function using the <code class="docutils literal notranslate"><span class="pre">sample_weight</span></code> argument. All examples are weighted equally by default.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-python notranslate"><div class="highlight"><pre>
<span></span><span class="n">x</span> <span class="o">=</span> <span class="n">nd</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">2</span><span class="p">,))</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">nd</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">2</span><span class="p">,))</span> <span class="o">*</span> <span class="mi">2</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">gloss</span><span class="o">.</span><span class="n">L2Loss</span><span class="p">()</span>
<span class="n">loss</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">nd</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">]))</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="Conclusion">
<h2>Conclusion<a class="headerlink" href="#Conclusion" title="Permalink to this headline">¶</a></h2>
<p>In this tutorial we saw an example of how to evaluate model performance using loss functions (during the forward pass). Crucially, we then saw how calculate parameter gradients (using <code class="docutils literal notranslate"><span class="pre">backward</span></code>) which would minimise this loss. You should now have a better understanding of when to apply different loss functions, especially for regression vs classification tasks.</p>
</div>
<div class="section" id="Recommended-Next-Steps">
<h2>Recommended Next Steps<a class="headerlink" href="#Recommended-Next-Steps" title="Permalink to this headline">¶</a></h2>
<p>In addition to loss functions, which are used for explicit optimization, you might want to look at metrics that give useful evaluation feedback even if they’re not explicitly optimized for in the same way as the loss. You might also want to learn more about the mechanics of the backpropagation stage in the autograd tutorial.</p>
</div>
</div>


        </div>
        <div class="side-doc-outline">
            <div class="side-doc-outline--content"> 
<div class="localtoc">
    <p class="caption">
      <span class="caption-text">Table Of Contents</span>
    </p>
    <ul>
<li><a class="reference internal" href="#">Loss functions</a><ul>
<li><a class="reference internal" href="#Basic-Usages">Basic Usages</a></li>
<li><a class="reference internal" href="#Loss-functions">Loss functions</a><ul>
<li><a class="reference internal" href="#L1-Loss">L1 Loss</a></li>
<li><a class="reference internal" href="#L2-Loss">L2 Loss</a></li>
<li><a class="reference internal" href="#Huber-Loss">Huber Loss</a></li>
<li><a class="reference internal" href="#Cross-Entropy-Loss-with-Sigmoid">Cross Entropy Loss with Sigmoid</a></li>
<li><a class="reference internal" href="#Cross-Entropy-Loss-with-Softmax">Cross Entropy Loss with Softmax</a></li>
<li><a class="reference internal" href="#Hinge-Loss">Hinge Loss</a></li>
<li><a class="reference internal" href="#Logistic-Loss">Logistic Loss</a></li>
<li><a class="reference internal" href="#Kullback-Leibler-Divergence-Loss">Kullback-Leibler Divergence Loss</a></li>
<li><a class="reference internal" href="#Triplet-Loss">Triplet Loss</a></li>
<li><a class="reference internal" href="#CTC-Loss">CTC Loss</a></li>
<li><a class="reference internal" href="#Cosine-Embedding-Loss">Cosine Embedding Loss</a></li>
<li><a class="reference internal" href="#PoissonNLLLoss">PoissonNLLLoss</a></li>
</ul>
</li>
<li><a class="reference internal" href="#Advanced:-Weighted-Loss">Advanced: Weighted Loss</a></li>
<li><a class="reference internal" href="#Conclusion">Conclusion</a></li>
<li><a class="reference internal" href="#Recommended-Next-Steps">Recommended Next Steps</a></li>
</ul>
</li>
</ul>

</div>
            </div>
        </div>                    

      <div class="clearer"></div>
    </div><div class="pagenation">
  </div>
            <footer class="site-footer h-card">
    <div class="wrapper">
        <div class="row">
            <div class="col-4">
                <h4 class="footer-category-title">Resources</h4>
                <ul class="contact-list">
                    <li><a class="u-email" href="mailto:dev@mxnet.apache.org">Dev list</a></li>
                    <li><a class="u-email" href="mailto:user@mxnet.apache.org">User mailing list</a></li>
                    <li><a href="https://cwiki.apache.org/confluence/display/MXNET/Apache+MXNet+Home">Developer Wiki</a></li>
                    <li><a href="https://issues.apache.org/jira/projects/MXNET/issues">Jira Tracker</a></li>
                    <li><a href="https://github.com/apache/incubator-mxnet/labels/Roadmap">Github Roadmap</a></li>
                    <li><a href="https://discuss.mxnet.io">MXNet Discuss forum</a></li>
                    <li><a href="/mxnet.io-v2/community/contribute">Contribute To MXNet</a></li>

                </ul>
            </div>

            <div class="col-4"><ul class="social-media-list"><li><a href="https://github.com/apache/incubator-mxnet"><svg class="svg-icon"><use xlink:href="../../../../_static/minima-social-icons.svg#github"></use></svg> <span class="username">apache/incubator-mxnet</span></a></li><li><a href="https://www.twitter.com/apachemxnet"><svg class="svg-icon"><use xlink:href="../../../../_static/minima-social-icons.svg#twitter"></use></svg> <span class="username">apachemxnet</span></a></li><li><a href="https://youtube.com/apachemxnet"><svg class="svg-icon"><use xlink:href="../../../../_static/minima-social-icons.svg#youtube"></use></svg> <span class="username">apachemxnet</span></a></li></ul>
</div>

            <div class="col-4 footer-text">
                <p>A flexible and efficient library for deep learning.</p>
            </div>
        </div>
    </div>
</footer>

<footer class="site-footer2">
    <div class="wrapper">
        <div class="row">
            <div class="col-3">
                <img src="../../../../_static/apache_incubator_logo.png" class="footer-logo col-2">
            </div>
            <div class="footer-bottom-warning col-9">
                <p>Apache MXNet is an effort undergoing incubation at The Apache Software Foundation (ASF), <span style="font-weight:bold">sponsored by the <i>Apache Incubator</i></span>. Incubation is required
                    of all newly accepted projects until a further review indicates that the infrastructure,
                    communications, and decision making process have stabilized in a manner consistent with other
                    successful ASF projects. While incubation status is not necessarily a reflection of the completeness
                    or stability of the code, it does indicate that the project has yet to be fully endorsed by the ASF.
                </p><p>"Copyright © 2017-2018, The Apache Software Foundation Apache MXNet, MXNet, Apache, the Apache
                    feather, and the Apache MXNet project logo are either registered trademarks or trademarks of the
                    Apache Software Foundation."</p>
            </div>
        </div>
    </div>
</footer>
        
  </body>
</html>